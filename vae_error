INFO:root:Parameters:
INFO:root:word_vec_size    :    128
INFO:root:share_embeddings    :    True
INFO:root:v_size    :    50000
INFO:root:encoder_type    :    rnn
INFO:root:decoder_type    :    rnn
INFO:root:enc_layers    :    2
INFO:root:dec_layers    :    1
INFO:root:encoder_size    :    256
INFO:root:decoder_size    :    512
INFO:root:dropout    :    0.0
INFO:root:classifier_dropout    :    0.1
INFO:root:residual    :    True
INFO:root:bidirectional    :    True
INFO:root:bridge    :    copy
INFO:root:attn_mode    :    concat
INFO:root:copy_attention    :    True
INFO:root:coverage_attn    :    False
INFO:root:review_attn    :    False
INFO:root:lambda_coverage    :    1.0
INFO:root:coverage_loss    :    False
INFO:root:orthogonal_loss    :    False
INFO:root:lambda_orthogonal    :    0.03
INFO:root:model_type    :    multi_view_multi_task_basic
INFO:root:classifier_type    :    word_multi_hop_attn
INFO:root:dec_classifier_type    :    word_multi_hop_attn
INFO:root:dec_classify_input_type    :    dec_state
INFO:root:rating_v_size    :    200
INFO:root:rating_memory_pred    :    False
INFO:root:rating_memory_type    :    gold
INFO:root:rating_bridge_type    :    None
INFO:root:query_hidden_size    :    512
INFO:root:detach_enc_logit_for_soft_feed    :    False
INFO:root:data    :    datasets/processed_reviews_Sports_and_Outdoors_5
INFO:root:train_from    :    
INFO:root:gpuid    :    -1
INFO:root:seed    :    250
INFO:root:delimiter    :    .
INFO:root:src_max_len    :    400
INFO:root:trg_max_len    :    100
INFO:root:epochs    :    50
INFO:root:start_epoch    :    1
INFO:root:w2v    :    word_embeddings/sports_and_outdoors/word2vec.128d.35k.bin
INFO:root:batch_size    :    32
INFO:root:batch_workers    :    0
INFO:root:max_grad_norm    :    2
INFO:root:loss_normalization    :    tokens
INFO:root:learning_rate    :    0.001
INFO:root:min_lr    :    1e-05
INFO:root:learning_rate_decay    :    0.5
INFO:root:start_checkpoint_at    :    0
INFO:root:start_decay_and_early_stop_at    :    2
INFO:root:checkpoint_interval    :    1000
INFO:root:disable_early_stop    :    False
INFO:root:early_stop_tolerance    :    4
INFO:root:timemark    :    20220312-102910
INFO:root:exp    :    train_movie_dual_view_inc_seed_250.ml.copy.bi-directional
INFO:root:exp_path    :    exp/train_movie_dual_view_inc_seed_250.ml.copy.bi-directional.20220312-102910
INFO:root:model_path    :    research/king3/ik_grp/wchen/senti_summ_models/saved_model/train_movie_dual_view_inc_seed_250.ml.copy.bi-directional.20220312-102910
INFO:root:num_classes    :    5
INFO:root:inconsistency_loss_type    :    KL_div
INFO:root:detach_dec_incosist_loss    :    False
INFO:root:detach_classify_dec_states    :    False
INFO:root:inconsistency_loss_weight    :    0.1
INFO:root:gen_loss_weight    :    0.8
INFO:root:class_loss_weight    :    0.1
INFO:root:class_loss_internal_enc_weight    :    1.0
INFO:root:class_loss_internal_dec_weight    :    1.0
INFO:root:weighted_sampling    :    False
INFO:root:weighted_classifier_loss    :    False
INFO:root:classifier_loss_type    :    xe
INFO:root:early_stop_loss    :    joint
INFO:root:stdout    :    False
INFO:root:ordinal    :    False
INFO:root:input_feeding    :    False
INFO:root:copy_input_feeding    :    False
INFO:root:device    :    cpu
INFO:root:Time for loading the data: 0.2
INFO:root:======================  Model Parameters  =========================
INFO:gensim.utils:loading Word2Vec object from word_embeddings/sports_and_outdoors/word2vec.128d.35k.bin
INFO:gensim.utils:loading wv recursively from word_embeddings/sports_and_outdoors/word2vec.128d.35k.bin.wv.* with mmap=None
INFO:gensim.utils:setting ignored attribute vectors_norm to None
INFO:gensim.utils:loading vocabulary recursively from word_embeddings/sports_and_outdoors/word2vec.128d.35k.bin.vocabulary.* with mmap=None
INFO:gensim.utils:loading trainables recursively from word_embeddings/sports_and_outdoors/word2vec.128d.35k.bin.trainables.* with mmap=None
INFO:gensim.utils:setting ignored attribute cum_table to None
INFO:gensim.utils:loaded word_embeddings/sports_and_outdoors/word2vec.128d.35k.bin
/home/parastiwari.rs.cse19.iitbhu/dual_view_review_sum-master/utils/io.py:512: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ../torch/csrc/utils/tensor_numpy.cpp:178.)
  embedding[i, :] = torch.Tensor(w2v['<s>'])
INFO:root:MultiViewMultiTaskBasicClassifySeq2Seq(
  (enc_classifier): WordMultiHopAttnClassifier(
    (attention_layer): Attention(
      (v): Linear(in_features=512, out_features=1, bias=False)
      (decode_project): Linear(in_features=512, out_features=512, bias=True)
      (memory_project): Linear(in_features=512, out_features=512, bias=False)
      (softmax): MaskedSoftmax()
      (tanh): Tanh()
    )
    (glimpse_layer): Attention(
      (v): Linear(in_features=512, out_features=1, bias=False)
      (decode_project): Linear(in_features=512, out_features=512, bias=True)
      (memory_project): Linear(in_features=512, out_features=512, bias=False)
      (softmax): MaskedSoftmax()
      (tanh): Tanh()
    )
    (classifier): Sequential(
      (0): Linear(in_features=512, out_features=512, bias=True)
      (1): Dropout(p=0.1, inplace=False)
      (2): ReLU()
      (3): Linear(in_features=512, out_features=5, bias=True)
      (4): LogSoftmax(dim=1)
    )
  )
  (dec_classifier): WordMultiHopAttnClassifier(
    (attention_layer): Attention(
      (v): Linear(in_features=512, out_features=1, bias=False)
      (decode_project): Linear(in_features=512, out_features=512, bias=True)
      (memory_project): Linear(in_features=512, out_features=512, bias=False)
      (softmax): MaskedSoftmax()
      (tanh): Tanh()
    )
    (glimpse_layer): Attention(
      (v): Linear(in_features=512, out_features=1, bias=False)
      (decode_project): Linear(in_features=512, out_features=512, bias=True)
      (memory_project): Linear(in_features=512, out_features=512, bias=False)
      (softmax): MaskedSoftmax()
      (tanh): Tanh()
    )
    (classifier): Sequential(
      (0): Linear(in_features=512, out_features=512, bias=True)
      (1): Dropout(p=0.1, inplace=False)
      (2): ReLU()
      (3): Linear(in_features=512, out_features=5, bias=True)
      (4): LogSoftmax(dim=1)
    )
  )
  (encoder): TwoLayerRNNEncoder(
    (embedding): Embedding(50004, 128, padding_idx=0)
    (dropout_layer): Dropout(p=0.0, inplace=False)
    (first_layer_rnn): GRU(128, 256, batch_first=True, bidirectional=True)
    (second_layer_rnn): GRU(512, 256, batch_first=True, bidirectional=True)
  )
  (decoder): MultiTaskBasicDecoder(
    (dropout): Dropout(p=0.0, inplace=False)
    (embedding): Embedding(50004, 128, padding_idx=0)
    (rnn): GRU(128, 512)
    (word_attention_layer): Attention(
      (v): Linear(in_features=512, out_features=1, bias=False)
      (decode_project): Linear(in_features=512, out_features=512, bias=True)
      (memory_project): Linear(in_features=512, out_features=512, bias=False)
      (softmax): MaskedSoftmax()
      (tanh): Tanh()
    )
    (p_gen_linear): Linear(in_features=1152, out_features=1, bias=True)
    (sigmoid): Sigmoid()
    (vocab_dist_linear_1): Linear(in_features=1024, out_features=512, bias=True)
    (vocab_dist_linear_2): Linear(in_features=512, out_features=50004, bias=True)
    (softmax): MaskedSoftmax()
  )
)
INFO:root:======================  Start Training  =========================
INFO:root:Epoch 1; batch: 0; total batch: 0
INFO:root:Epoch 1; batch: 1000; total batch: 1000
INFO:root:Enter check point!
INFO:root:Epoch: 1; batch idx: 1000; total batches: 1000
INFO:root:training enc class loss: 0.890; valid enc class loss: 0.838; valid enc class f1: 0.357
INFO:root:Valid loss drops
INFO:root:Saving checkpoint to research/king3/ik_grp/wchen/senti_summ_models/saved_model/train_movie_dual_view_inc_seed_250.ml.copy.bi-directional.20220312-102910/ckpt/train_movie_dual_view_inc_seed_250.ml.copy.bi-directional-epoch-1-total_batch-1000-joint-0.838
INFO:root:Epoch 1; batch: 2000; total batch: 2000
INFO:root:Enter check point!
INFO:root:Epoch: 1; batch idx: 2000; total batches: 2000
INFO:root:training enc class loss: 0.776; valid enc class loss: 0.734; valid enc class f1: 0.482
INFO:root:Valid loss drops
INFO:root:Saving checkpoint to research/king3/ik_grp/wchen/senti_summ_models/saved_model/train_movie_dual_view_inc_seed_250.ml.copy.bi-directional.20220312-102910/ckpt/train_movie_dual_view_inc_seed_250.ml.copy.bi-directional-epoch-1-total_batch-2000-joint-0.734
INFO:root:Epoch 1; batch: 3000; total batch: 3000
INFO:root:Enter check point!
INFO:root:Epoch: 1; batch idx: 3000; total batches: 3000
INFO:root:training enc class loss: 0.752; valid enc class loss: 0.720; valid enc class f1: 0.472
INFO:root:Valid loss drops
INFO:root:Saving checkpoint to research/king3/ik_grp/wchen/senti_summ_models/saved_model/train_movie_dual_view_inc_seed_250.ml.copy.bi-directional.20220312-102910/ckpt/train_movie_dual_view_inc_seed_250.ml.copy.bi-directional-epoch-1-total_batch-3000-joint-0.720
INFO:root:Epoch 1; batch: 4000; total batch: 4000
INFO:root:Enter check point!
INFO:root:Epoch: 1; batch idx: 4000; total batches: 4000
INFO:root:training enc class loss: 0.720; valid enc class loss: 0.715; valid enc class f1: 0.456
INFO:root:Valid loss drops
INFO:root:Saving checkpoint to research/king3/ik_grp/wchen/senti_summ_models/saved_model/train_movie_dual_view_inc_seed_250.ml.copy.bi-directional.20220312-102910/ckpt/train_movie_dual_view_inc_seed_250.ml.copy.bi-directional-epoch-1-total_batch-4000-joint-0.715
INFO:root:Epoch 1; batch: 5000; total batch: 5000
INFO:root:Enter check point!
INFO:root:Epoch: 1; batch idx: 5000; total batches: 5000
INFO:root:training enc class loss: 0.720; valid enc class loss: 0.691; valid enc class f1: 0.519
INFO:root:Valid loss drops
INFO:root:Saving checkpoint to research/king3/ik_grp/wchen/senti_summ_models/saved_model/train_movie_dual_view_inc_seed_250.ml.copy.bi-directional.20220312-102910/ckpt/train_movie_dual_view_inc_seed_250.ml.copy.bi-directional-epoch-1-total_batch-5000-joint-0.691
INFO:root:Epoch 2; batch: 258; total batch: 6000
INFO:root:Enter check point!
INFO:root:Epoch: 2; batch idx: 258; total batches: 6000
INFO:root:training enc class loss: 0.690; valid enc class loss: 0.723; valid enc class f1: 0.489
INFO:root:Valid loss does not drop
INFO:root:Learning rate drops to 0.0005
INFO:root:Epoch 2; batch: 1258; total batch: 7000
INFO:root:Enter check point!
INFO:root:Epoch: 2; batch idx: 1258; total batches: 7000
INFO:root:training enc class loss: 0.625; valid enc class loss: 0.691; valid enc class f1: 0.523
INFO:root:Valid loss drops
INFO:root:Saving checkpoint to research/king3/ik_grp/wchen/senti_summ_models/saved_model/train_movie_dual_view_inc_seed_250.ml.copy.bi-directional.20220312-102910/ckpt/train_movie_dual_view_inc_seed_250.ml.copy.bi-directional-epoch-2-total_batch-7000-joint-0.691
INFO:root:Epoch 2; batch: 2258; total batch: 8000
INFO:root:Enter check point!
INFO:root:Epoch: 2; batch idx: 2258; total batches: 8000
INFO:root:training enc class loss: 0.616; valid enc class loss: 0.680; valid enc class f1: 0.536
INFO:root:Valid loss drops
INFO:root:Saving checkpoint to research/king3/ik_grp/wchen/senti_summ_models/saved_model/train_movie_dual_view_inc_seed_250.ml.copy.bi-directional.20220312-102910/ckpt/train_movie_dual_view_inc_seed_250.ml.copy.bi-directional-epoch-2-total_batch-8000-joint-0.680
INFO:root:Epoch 2; batch: 3258; total batch: 9000
INFO:root:Enter check point!
INFO:root:Epoch: 2; batch idx: 3258; total batches: 9000
INFO:root:training enc class loss: 0.611; valid enc class loss: 0.691; valid enc class f1: 0.515
INFO:root:Valid loss does not drop
INFO:root:Learning rate drops to 0.00025
INFO:root:Epoch 2; batch: 4258; total batch: 10000
INFO:root:Enter check point!
INFO:root:Epoch: 2; batch idx: 4258; total batches: 10000
INFO:root:training enc class loss: 0.609; valid enc class loss: 0.679; valid enc class f1: 0.533
INFO:root:Valid loss drops
INFO:root:Saving checkpoint to research/king3/ik_grp/wchen/senti_summ_models/saved_model/train_movie_dual_view_inc_seed_250.ml.copy.bi-directional.20220312-102910/ckpt/train_movie_dual_view_inc_seed_250.ml.copy.bi-directional-epoch-2-total_batch-10000-joint-0.679
INFO:root:Epoch 2; batch: 5258; total batch: 11000
INFO:root:Enter check point!
INFO:root:Epoch: 2; batch idx: 5258; total batches: 11000
INFO:root:training enc class loss: 0.608; valid enc class loss: 0.674; valid enc class f1: 0.542
INFO:root:Valid loss drops
INFO:root:Saving checkpoint to research/king3/ik_grp/wchen/senti_summ_models/saved_model/train_movie_dual_view_inc_seed_250.ml.copy.bi-directional.20220312-102910/ckpt/train_movie_dual_view_inc_seed_250.ml.copy.bi-directional-epoch-2-total_batch-11000-joint-0.674
INFO:root:Epoch 3; batch: 516; total batch: 12000
INFO:root:Enter check point!
INFO:root:Epoch: 3; batch idx: 516; total batches: 12000
INFO:root:training enc class loss: 0.561; valid enc class loss: 0.720; valid enc class f1: 0.527
INFO:root:Valid loss does not drop
INFO:root:Learning rate drops to 0.000125
INFO:root:Epoch 3; batch: 1516; total batch: 13000
INFO:root:Enter check point!
INFO:root:Epoch: 3; batch idx: 1516; total batches: 13000
INFO:root:training enc class loss: 0.510; valid enc class loss: 0.699; valid enc class f1: 0.548
INFO:root:Valid loss drops
INFO:root:Saving checkpoint to research/king3/ik_grp/wchen/senti_summ_models/saved_model/train_movie_dual_view_inc_seed_250.ml.copy.bi-directional.20220312-102910/ckpt/train_movie_dual_view_inc_seed_250.ml.copy.bi-directional-epoch-3-total_batch-13000-joint-0.699
slurmstepd: error: *** JOB 723854 ON cn006 CANCELLED AT 2022-03-14T08:23:18 ***
